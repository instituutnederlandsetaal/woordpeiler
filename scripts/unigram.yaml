---

# Number of docs to process in parallel per run. After each run,
# we check if we need to write to chunk file.
# Larger values allow more parallellism but risk overshooting the
# chunk file size target.
# Default: 100_000
docsToProcessInParallel: 10_000

# How large to grow the grouping until we write the intermediate result to disk.
# Higher values decrease processing overhead but increase memory requirements.
# Default: 5_000_000
groupsPerChunk: 5_000_000

# The annotated field to make frequency lists for
annotatedField: contents

# The frequency lists we want to make
frequencyLists:

  - annotations:
      - lemma
      - word
      - pos_full
    metadataFields:
      - witnessDate_from
      - titleLevel2
      - languageVariant
    name: "unigram"
    filter: "medium:newspaper AND (titleLevel2_untokenized:(NRC) OR titleLevel2_untokenized:(Het Parool) OR titleLevel2_untokenized:(De Volkskrant) OR titleLevel2_untokenized:(Trouw) OR titleLevel2_untokenized:(Algemeen Dagblad) OR titleLevel2_untokenized:(Het Nieuwsblad) OR titleLevel2_untokenized:(De Standaard) OR titleLevel2_untokenized:(Het Belang van Limburg) OR titleLevel2_untokenized:(Gazet van Antwerpen) OR titleLevel2_untokenized:(Het Laatste Nieuws) OR titleLevel2_untokenized:(De Morgen) OR titleLevel2_untokenized:(Starnieuws) OR titleLevel2_untokenized:(Antilliaans Dagblad) OR titleLevel2_untokenized:(Bonaire.nu) OR titleLevel2_untokenized:(Bonaire.Nu))"
# newspaper selection was made with the following query:
# SELECT source FROM source_frequencies JOIN sources ON source_id=id WHERE time >= '2024-01-01' GROUP BY source_id, source;
# and manual additions for AN and SN.
